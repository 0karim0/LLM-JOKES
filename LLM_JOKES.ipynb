{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNST6RR7yXBJ6g2n5Xtfb4C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0karim0/LLM-JOKES/blob/main/LLM_JOKES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding API keys to your .env file\n",
        "## When you get your API keys, you need to set them as environment variables by adding them to your .env file.\n",
        "\n",
        "### OPENAI_API_KEY=xxxx\n",
        "### ANTHROPIC_API_KEY=xxxx\n",
        "### GOOGLE_API_KEY=xxxx"
      ],
      "metadata": {
        "id": "f4W2W3WxYMkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from openai import OpenAI\n",
        "import anthropic\n",
        "import google.generativeai\n",
        "\n",
        "from IPython.display import Markdown, display, update_display"
      ],
      "metadata": {
        "id": "-xu8w6CiYOQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables in a file called .env\n",
        "\n",
        "load_dotenv(override=True)\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n"
      ],
      "metadata": {
        "id": "VfjdhOfFYzGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LO9U0c5vY9BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai = OpenAI()\n",
        "\n",
        "claude = anthropic.Anthropic()\n",
        "\n",
        "google.generativeai.configure()"
      ],
      "metadata": {
        "id": "6ObjNdfBY9Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Asking LLMs to tell a joke\n",
        "###### It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models. Later we will be putting LLMs to better use!"
      ],
      "metadata": {
        "id": "EuCZv2RRZKip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an assistant that is great at telling jokes\"\n",
        "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
      ],
      "metadata": {
        "id": "OSqUHZwzZKCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]"
      ],
      "metadata": {
        "id": "DIKPCOElZTvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-3.5-Turbo\n",
        "\n",
        "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "2UBWtjjTZT86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-4o-mini\n",
        "# Temperature setting controls creativity\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=prompts,\n",
        "    temperature=0.7\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "knQFOz0FZZhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-4o\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "    model='gpt-4o',\n",
        "    messages=prompts,\n",
        "    temperature=0.4\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "81WmARY3Zb1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Claude 3.5 Sonnet\n",
        "\n",
        "message = claude.messages.create(\n",
        "    model=\"claude-3-5-sonnet-latest\",\n",
        "    max_tokens=200,\n",
        "    temperature=0.7,\n",
        "    system=system_message,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(message.content[0].text)"
      ],
      "metadata": {
        "id": "ZLbY0-PZZhPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Claude 3.5 Sonnet again\n",
        "\n",
        "result = claude.messages.stream(\n",
        "    model=\"claude-3-5-sonnet-latest\",\n",
        "    max_tokens=200,\n",
        "    temperature=0.7,\n",
        "    system=system_message,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ],\n",
        ")\n",
        "\n",
        "with result as stream:\n",
        "    for text in stream.text_stream:\n",
        "            print(text, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "3zkd1bIiZm_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemini = google.generativeai.GenerativeModel(\n",
        "    model_name='gemini-2.0-flash-exp',\n",
        "    system_instruction=system_message\n",
        ")\n",
        "response = gemini.generate_content(user_prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "cMZuCsgHaLvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
        "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
        "\n",
        "gemini_via_openai_client = OpenAI(\n",
        "    api_key=google_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "response = gemini_via_openai_client.chat.completions.create(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    messages=prompts\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "TBTks-PJaMhz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}